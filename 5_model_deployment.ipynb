{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d07e1508-e824-4d4b-9798-f6485066854c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "emotional_words = [\n",
    "    'ecstatic', 'elated', 'blissful', 'jubilant', 'cheerful', 'thrilled', 'delight', 'exhilarated',\n",
    "    'content', 'grateful', 'affection', 'adoration', 'devotion', 'tenderness', 'caring', 'compassionate',\n",
    "    'empathy', 'cherish', 'admire', 'fondness', 'hopeful', 'confident', 'optimistic', 'encouraged',\n",
    "    'reassured', 'inspired', 'ambitious', 'enthusiastic', 'dreamy', 'faithful',\n",
    "    'terrified', 'horrified', 'anxious', 'panicked', 'frightened', 'apprehensive', 'worried', 'nervous',\n",
    "    'alarmed', 'dread', 'furious', 'outraged', 'enraged', 'livid', 'infuriated', 'annoyed', 'frustrated',\n",
    "    'irritated', 'exasperated', 'agitated', 'heartbroken', 'devastated', 'mournful', 'sorrowful',\n",
    "    'melancholy', 'dejected', 'despair', 'loneliness', 'hopeless', 'disheartened',\n",
    "    'amazed', 'stunned', 'shocked', 'astounded', 'astonished', 'startled', 'dumbfounded', 'speechless',\n",
    "    'perplexed', 'bewildered',\n",
    "    'disgusted', 'repulsed', 'nauseated', 'revolted', 'loathing', 'detest', 'despise', 'scorn',\n",
    "    'contemptuous', 'abhor',\n",
    "    'ashamed', 'embarrassed', 'guilty', 'humiliated', 'regretful', 'remorseful', 'mortified', \n",
    "    'self-conscious', 'apologetic', 'disgraced'\n",
    "]\n",
    "\n",
    "def calculate_phrase_mean_length(texts):\n",
    "    phrase_mean_length = []\n",
    "\n",
    "    for t in texts:\n",
    "        # Split text into phrases (by period)\n",
    "        phrases = [phrase.strip() for phrase in t.split('.')]  \n",
    "\n",
    "        # Get the length of each phrase in terms of word count\n",
    "        phrase_lengths = [len(phrase.split()) for phrase in phrases if phrase]  \n",
    "\n",
    "        # Calculate the mean length of the phrases\n",
    "        if phrase_lengths:  \n",
    "            mean_length = sum(phrase_lengths) / len(phrase_lengths)\n",
    "        else:\n",
    "            mean_length = 0\n",
    "\n",
    "        phrase_mean_length.append(mean_length)\n",
    "    \n",
    "    return phrase_mean_length\n",
    "\n",
    "def count_numbers_in_texts(texts):\n",
    "    number_counts = []\n",
    "\n",
    "    for text in texts:\n",
    "        # Use regular expression to find all sequences of digits\n",
    "        numbers = re.findall(r'\\d+', text)\n",
    "        \n",
    "        # Count the number of sequences found\n",
    "        number_count = len(numbers)\n",
    "        \n",
    "        number_counts.append(number_count)\n",
    "    \n",
    "    return number_counts\n",
    "\n",
    "def count_proper_nouns(texts):\n",
    "    proper_noun_counts = []\n",
    "\n",
    "    for text in texts:\n",
    "        # Split text into sentences using punctuation as delimiters\n",
    "        sentences = re.split(r'[.!?]\\s+', text)\n",
    "\n",
    "        proper_noun_count = 0\n",
    "\n",
    "        # For each sentence, find capitalized words that are NOT at the start\n",
    "        for sentence in sentences:\n",
    "            # Split sentence into words\n",
    "            words = sentence.split()\n",
    "            \n",
    "            # Exclude the first word in the sentence and count proper nouns\n",
    "            proper_nouns = [word for word in words[1:] if re.match(r'\\b[A-Z][a-z]*\\b', word)]\n",
    "            \n",
    "            proper_noun_count += len(proper_nouns)\n",
    "        \n",
    "        proper_noun_counts.append(proper_noun_count)\n",
    "    \n",
    "    return proper_noun_counts\n",
    "\n",
    "def count_punctuation_signals(texts):\n",
    "    punctuation_counts = []\n",
    "\n",
    "    # Define a regular expression for common punctuation marks\n",
    "    punctuation_pattern = r'[.,!?;:()\\[\\]\\'\\\"-]'\n",
    "\n",
    "    for text in texts:\n",
    "        # Find all punctuation marks in the text\n",
    "        punctuation_signals = re.findall(punctuation_pattern, text)\n",
    "        \n",
    "        # Count the number of punctuation marks\n",
    "        punctuation_count = len(punctuation_signals)\n",
    "        \n",
    "        punctuation_counts.append(punctuation_count)\n",
    "    \n",
    "    return punctuation_counts\n",
    "\n",
    "def count_stopwords(texts):\n",
    "    stopword_counts = []\n",
    "\n",
    "    for text in texts:\n",
    "        # Tokenize the text into words, lowercased\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        \n",
    "        # Count the number of stopwords\n",
    "        stopword_count = sum(1 for word in words if word in nltk_stopwords)\n",
    "        \n",
    "        stopword_counts.append(stopword_count)\n",
    "    \n",
    "    return stopword_counts\n",
    "\n",
    "def count_emotional_words(text):\n",
    "    # Ensure text is a string and normalize: remove punctuation and convert to lowercase\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    \n",
    "    # Count how many emotional words are in the text\n",
    "    emotional_word_count = sum(1 for word in words if word in emotional_words)\n",
    "    \n",
    "    return emotional_word_count\n",
    "\n",
    "def count_repeated_non_stopwords(text, threshold=10):\n",
    "    # Load English stopwords from NLTK\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Normalize the text: remove punctuation and convert to lowercase\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    \n",
    "    # Filter out stopwords\n",
    "    non_stopwords = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Count the frequency of each non-stopword\n",
    "    word_counts = Counter(non_stopwords)\n",
    "    \n",
    "    # Filter words repeated more than 'threshold' times\n",
    "    repeated_words = [word for word, count in word_counts.items() if count > threshold]\n",
    "    \n",
    "    # Return the count of non-stopwords repeated more than 'threshold' times\n",
    "    return len(repeated_words)\n",
    "\n",
    "def count_capital_letters(text):\n",
    "    # Count the number of uppercase letters in the text\n",
    "    return sum(1 for char in text if char.isupper())\n",
    "\n",
    "def count_references(text):\n",
    "    # Define common patterns for references:\n",
    "    reference_patterns = [\n",
    "        r'\\[\\d+\\]',                          # Numeric references like [1], [23]\n",
    "        r'\\(\\d+\\)',                          # Numeric references like (1), (23)\n",
    "        r'\\([A-Za-z]+, \\d{4}\\)',             # Parenthetical academic references like (Smith, 2020)\n",
    "        r'\\b\\d{4}\\b',                        # Standalone years like 2020, 2019\n",
    "        r'\\bdoi:?\\s?10\\.\\d{4,9}/[-._;()/:A-Z0-9]+\\b',  # DOI references (e.g., doi:10.1234/abcd.5678)\n",
    "        r'(https?://[^\\s]+)',                # URLs (e.g., http://example.com or https://doi.org)\n",
    "        r'\\bISBN[-\\s]?(?:\\d{9}[\\dXx]|\\d{13})\\b',  # ISBN numbers (ISBN-10 or ISBN-13 format)\n",
    "        r'\\^[1-9]\\d*',                       # Footnotes like ^1, ^2, etc.\n",
    "        r'[A-Za-z]+ et al\\., \\d{4}',         # Common academic style, e.g., Smith et al., 2020\n",
    "        r'\\([A-Za-z]+ et al\\., \\d{4}\\)'      # Full academic reference style, e.g., (Smith et al., 2020)\n",
    "    ]\n",
    "    \n",
    "    # Combine all patterns into a single regex pattern\n",
    "    combined_pattern = '|'.join(reference_patterns)\n",
    "    \n",
    "    # Find all matches in the text\n",
    "    references = re.findall(combined_pattern, text)\n",
    "    \n",
    "    return len(references)\n",
    "\n",
    "def mean_word_length(text):\n",
    "    # Normalize the text by removing punctuation and extracting words\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    \n",
    "    # Calculate the lengths of the words\n",
    "    word_lengths = [len(word) for word in words]\n",
    "    \n",
    "    # Calculate the mean (average) word length\n",
    "    if word_lengths:  # Ensure there are words to avoid division by zero\n",
    "        mean_length = sum(word_lengths) / len(word_lengths)\n",
    "    else:\n",
    "        mean_length = 0\n",
    "    \n",
    "    return mean_length\n",
    "\n",
    "\n",
    "def get_features(title, text):\n",
    "    columns = np.zeros(13)\n",
    "    #text title length\n",
    "    columns[0] = len(text)\n",
    "    columns[1] = len(title)\n",
    "    # Length of phrases (mean length)\n",
    "    columns[2] = np.mean(calculate_phrase_mean_length([text]))  # Calculate mean for the text\n",
    "    columns[3] = np.mean(calculate_phrase_mean_length([title]))  # Calculate mean for the title\n",
    "    # Quantity of numbers in the text\n",
    "    columns[4] = count_numbers_in_texts([text])[0]  # Call function and take the first item\n",
    "    # Quantity of proper nouns in the text\n",
    "    columns[5] = count_proper_nouns([text])[0]  # Call function and take the first item\n",
    "    # Quantity of punctuation signals in the text\n",
    "    columns[6] = count_punctuation_signals([text])[0]  # Call function and take the first item\n",
    "    # Quantity of stopwords in the text\n",
    "    columns[7] = count_stopwords([text])[0]  # Call function and take the first item\n",
    "    #emotional words\n",
    "    columns[8] = count_emotional_words(text)\n",
    "    #repeated non stopwords\n",
    "    columns[9] = count_repeated_non_stopwords(text, threshold=10)\n",
    "    #capital letters title\n",
    "    columns[10] = count_capital_letters(title)\n",
    "    #references\n",
    "    columns[11] = count_references(text)\n",
    "    #word length\n",
    "    columns[12] = mean_word_length(text)\n",
    "\n",
    "    # get dataframe\n",
    "    headers= ['text_length', 'title_length', 'length_phrases_no_punctuation_text',\n",
    "       'length_phrases_no_punctuation_title', 'qtt_numbers', 'qtt_noms_propis',\n",
    "       'qtt_punt', 'stopwords', 'emotional', 'repeated', 'capital_title',\n",
    "       'reference_count', 'word_length']\n",
    "\n",
    "    # Create a DataFrame with shape (1, 13)\n",
    "    df = pd.DataFrame(columns.reshape(1, -1), columns=headers)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "df = get_features('e','fake new')\n",
    "\n",
    "# Scale the data\n",
    "robust_scaler = joblib.load('robust_scaler.pkl') \n",
    "\n",
    "scaled_input = robust_scaler.transform(df)\n",
    "\n",
    "#load the trained model\n",
    "model = joblib.load('final_logistic_regression_model.pkl')\n",
    "\n",
    "model.predict(scaled_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "193b392f-5b78-43e4-ab55-4da160c0be23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 13)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_features('deded', 'e')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f1cd47a4-0abf-48ca-acdb-609546b91427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "robust_scaler = joblib.load('robust_scaler.pkl') \n",
    "\n",
    "scaled_input = robust_scaler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8fe3d122-a54c-4cc4-b1ad-0d739f9ebfac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the trained model\n",
    "model = joblib.load('final_logistic_regression_model.pkl')\n",
    "\n",
    "model.predict(scaled_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87ab7ee-8849-44a3-831d-42ec735b05ae",
   "metadata": {},
   "source": [
    "# FastAPI deployment\n",
    "\n",
    "we will do that in an `app.py` file instead of a jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf64fe14-60c1-46b1-bb94-28372f8b4814",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
